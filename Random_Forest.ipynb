{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar tokenizer y embeddings\n",
    "\n",
    "modelo_bert  = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(modelo_bert)\n",
    "modelo  = BertModel.from_pretrained(modelo_bert)\n",
    "modelo .eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir palabras a partir de subtokens\n",
    "\n",
    "def reconstruir_palabras(df):\n",
    "    \"\"\"\n",
    "    Recibe el df de tokens y devuelve un df con una columna 'palabra_id'\n",
    "    que identifica a qué palabra pertenece cada token.\n",
    "    \"\"\"\n",
    "    df[\"token\"] = df[\"token_id\"].apply(tokenizer.convert_ids_to_tokens)\n",
    "    \n",
    "    ids_palabra = []\n",
    "    id_actual  = 0\n",
    "\n",
    "    for id_instancia, grupo in df.groupby(\"instancia_id\"):\n",
    "        ids = []\n",
    "        id_actual = 0\n",
    "\n",
    "        for i, tok in enumerate(grupo[\"token\"]):\n",
    "            if tok.startswith(\"##\"):\n",
    "                # mismo palabra_id que el anterior\n",
    "                ids.append(id_actual)\n",
    "            else:\n",
    "                # empieza palabra nueva\n",
    "                id_actual += 1\n",
    "                ids.append(id_actual)\n",
    "\n",
    "        ids_palabra.extend(ids)\n",
    "\n",
    "    df[\"palabra_id\"] = ids_palabra\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir features a nivel palabra\n",
    "\n",
    "def crear_features(df, nombres_propios=None):\n",
    "    \"\"\"\n",
    "    Crea features por palabra y luego las copia por token.\n",
    "    nombres_propios: conjunto con palabras consideradas nombres propios (opcional)\n",
    "    \"\"\"\n",
    "\n",
    "    if nombres_propios is None:\n",
    "        nombres_propios = set()  # si no tenés nada todavía\n",
    "\n",
    "    # reconstruir palabra concatenando subtokens\n",
    "    def unir_subtokens(grupo):\n",
    "        palabra  = \"\"\n",
    "        for tok in grupo[\"token\"].tolist():\n",
    "            if tok.startswith(\"##\"):\n",
    "                palabra += tok[2:]\n",
    "            else:\n",
    "                palabra += tok\n",
    "        return palabra\n",
    "\n",
    "    palabras = df.groupby([\"instancia_id\", \"palabra_id\"]).apply(unir_subtokens).reset_index()\n",
    "    palabras.columns = [\"instancia_id\", \"palabra_id\", \"palabra\"]\n",
    "    df = df.merge(palabras, on=[\"instancia_id\", \"palabra_id\"], how=\"left\")\n",
    "\n",
    "    # Posición en el texto (por palabra)\n",
    "    df[\"posicion_palabra\"] = df.groupby(\"instancia_id\")[\"palabra_id\"].transform(\n",
    "        lambda x: x.rank(method=\"dense\").astype(int)\n",
    "    )\n",
    "\n",
    "    # Cantidad_palabras - posición\n",
    "    df[\"total_palabras\"] = df.groupby(\"instancia_id\")[\"palabra_id\"].transform(\"max\")\n",
    "    df[\"dist_al_final\"] = df[\"total_palabras\"] - df[\"posicion_palabra\"]\n",
    "\n",
    "    # Longitud de la palabra\n",
    "    df[\"longitud_palabra\"] = df[\"palabra\"].str.len()\n",
    "\n",
    "    # Frecuencia de la palabra en la instancia\n",
    "    freq = df.groupby([\"instancia_id\", \"palabra\"])[\"palabra\"].transform(\"count\")\n",
    "    df[\"frecuencia_en_instancia\"] = freq\n",
    "\n",
    "    # es_nombre?\n",
    "    df[\"es_nombre\"] = df[\"palabra\"].isin(nombres_propios).astype(int)\n",
    "\n",
    " \n",
    "    # Embeddings a nivel palabra\n",
    "    \n",
    "    def obtener_embedding_subtoken(token_id):\n",
    "        \"\"\"Embedding del subtoken.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            return modelo.embeddings.word_embeddings.weight[token_id].cpu().numpy()\n",
    "\n",
    "    # obtener embedding por subtoken\n",
    "    df[\"embedding_subtoken\"] = df[\"token_id\"].apply(obtener_embedding_subtoken)\n",
    "\n",
    "    # promediar embeddings de subtokens por palabra\n",
    "    embeddings_palabra  = (\n",
    "        df.groupby([\"instancia_id\", \"palabra_id\"])[\"embedding_subtoken\"]\n",
    "        .apply(lambda xs: np.mean(np.vstack(xs.values), axis=0))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    embeddings_palabra.columns = [\"instancia_id\", \"palabra_id\", \"embedding_palabra\"]\n",
    "\n",
    "    df = df.merge(embeddings_palabra, on=[\"instancia_id\", \"palabra_id\"], how=\"left\")\n",
    "\n",
    "    # eliminar columna auxiliar\n",
    "    df.drop(columns=[\"embedding_subtoken\"], inplace=True)\n",
    "\n",
    "    # Distancia coseno a palabra anterior y siguiente\n",
    "    def similitud_coseno(a, b):\n",
    "        a = torch.tensor(a)\n",
    "        b = torch.tensor(b)\n",
    "        return F.cosine_similarity(a, b, dim=0).item()\n",
    "\n",
    "    df[\"embedding_anterior\"] = df.groupby(\"instancia_id\")[\"embedding_palabra\"].shift(1)\n",
    "    df[\"embedding_siguiente\"] = df.groupby(\"instancia_id\")[\"embedding_palabra\"].shift(-1)\n",
    "\n",
    "    def calcular_similitud(fila, columna):\n",
    "        valor = fila[columna]\n",
    "        if valor is None or (isinstance(valor, float) and np.isnan(valor)):\n",
    "            return 0\n",
    "        return similitud_coseno(fila[\"embedding_palabra\"], fila[columna])\n",
    "\n",
    "    df[\"similitud_con_anterior\"] = df.apply(lambda r: calcular_similitud(r, \"embedding_anterior\"), axis=1)\n",
    "    df[\"similitud_con_siguiente\"] = df.apply(lambda r: calcular_similitud(r, \"embedding_siguiente\"), axis=1)\n",
    "\n",
    "    # Quitar embeddings auxiliares\n",
    "    df.drop(columns=[\"embedding_anterior\", \"embedding_siguiente\"], inplace=True)\n",
    "    df.drop(columns=[\"embedding_palabra\"], inplace=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Crear features a nivel token (fusionando features palabra)\n",
    "\n",
    "def crear_features_token(df):\n",
    "    \"\"\"\n",
    "    Recibe df que ya tiene las columnas por token + las columnas\n",
    "    a nivel palabra (palabra, posicion_palabra, dist_al_final, longitud_palabra,\n",
    "    frecuencia_en_instancia, es_nombre, similitud_con_anterior, similitud_con_siguiente (opc))\n",
    "    Devuelve df_tokens con features listas para RF de puntuacion (por token).\n",
    "    \"\"\"\n",
    "\n",
    "    # -- Copiamos para no mutar original --\n",
    "    df_tok = df.copy()\n",
    "\n",
    "    # -- marcar subtokens --\n",
    "    df_tok[\"is_subtoken\"] = df_tok[\"token\"].str.startswith(\"##\").astype(int)\n",
    "\n",
    "    # índice del token dentro de la palabra (1 = primer subtoken)\n",
    "    def idx_en_palabra(gr):\n",
    "        # devuelve índice por el orden de aparición en el grupo\n",
    "        return np.arange(1, len(gr) + 1)\n",
    "\n",
    "    df_tok[\"token_idx_en_palabra\"] = df_tok.groupby(\n",
    "        [\"instancia_id\", \"palabra_id\"]).cumcount() + 1\n",
    "\n",
    "    # tamaño (longitud) del token textual (sin ##)\n",
    "    df_tok[\"token_text\"] = df_tok[\"token\"].str.replace(\"^##\", \"\", regex=True)\n",
    "    df_tok[\"longitud_token\"] = df_tok[\"token_text\"].str.len()\n",
    "\n",
    "    # es primer subtoken de la palabra?\n",
    "    df_tok[\"es_primer_subtoken\"] = (df_tok[\"token_idx_en_palabra\"] == 1).astype(int)\n",
    "\n",
    "    # es ultimo subtoken -> se puede calcular comparando token_idx con cantidad de subtokens por palabra\n",
    "    subtoks_por_palabra = df_tok.groupby([\"instancia_id\", \"palabra_id\"])[\"token\"].transform(\"count\")\n",
    "    df_tok[\"es_ultimo_subtoken\"] = (df_tok[\"token_idx_en_palabra\"] == subtoks_por_palabra).astype(int)\n",
    "    \n",
    "\n",
    "    # Features basadas en la palabra (ya deben existir en df): las copiamos (si no existen, créalas primero)\n",
    "    # Suponemos que df ya tiene: posicion_palabra, dist_al_final, longitud_palabra, frecuencia_en_instancia, es_nombre\n",
    "    # (si no, corre crear_features_palabra antes)\n",
    "    # nada que hacer: las columnas ya están por token porque las generaste a nivel palabra y mergeaste al df.\n",
    "\n",
    "    # Features de contexto rápido (sin embeddings):\n",
    "    # indicar si la palabra anterior / siguiente es nombre propio (útil para puntuación)\n",
    "    df_tok[\"palabra_anterior_es_nombre\"] = df_tok.groupby(\"instancia_id\")[\"es_nombre\"].shift(1).fillna(0).astype(int)\n",
    "    df_tok[\"palabra_siguiente_es_nombre\"] = df_tok.groupby(\"instancia_id\")[\"es_nombre\"].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "    # Distancia en caracteres al inicio/fin del token dentro de la palabra (opcional)\n",
    "    # si querés: posición relativa del subtoken dentro de la palabra\n",
    "    df_tok[\"token_pos_relativa\"] = df_tok[\"token_idx_en_palabra\"] / subtoks_por_palabra\n",
    "\n",
    "    # Eliminar columnas intermedias si querés\n",
    "    df_tok.drop(columns=[\"token_text\"], inplace=True)\n",
    "\n",
    "    return df_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de nombres y apellidos\n",
    "\n",
    "nombres_y_apellidos = {\n",
    "\n",
    "    # Nombres masculinos\n",
    "    \"juan\",\"jose\",\"josé\",\"javier\",\"jorge\",\"julian\",\"julían\",\"julio\",\"joel\",\n",
    "    \"joaquin\",\"joaquín\",\"miguel\",\"martin\",\"martín\",\"marco\",\"marcos\",\"mateo\",\n",
    "    \"matias\",\"matías\",\"maximiliano\",\"manuel\",\"mariano\",\"mauricio\",\"mirko\",\n",
    "    \"nahuel\",\"nicolas\",\"nicólas\",\"nazareno\",\"pablo\",\"pedro\",\"patricio\",\n",
    "    \"ramiro\",\"ricardo\",\"roberto\",\"rodrigo\",\"román\",\"santiago\",\"sergio\",\n",
    "    \"sebastian\",\"sebastián\",\"samuel\",\"tomás\",\"tomas\",\"thiago\",\"tiago\",\n",
    "    \"ulises\",\"victor\",\"víctor\",\"valentin\",\"valentín\",\"william\",\"walter\",\n",
    "    \"xavier\",\"yago\",\"yamil\",\"zaid\",\"zair\",\"zahir\",\n",
    "\n",
    "    # Nombres femeninos\n",
    "    \"ana\",\"andrea\",\"antonella\",\"antonela\",\"agustina\",\"belén\",\"belen\",\"brenda\",\n",
    "    \"brisa\",\"bárbara\",\"barbara\",\"camila\",\"celeste\",\"carolina\",\"candela\",\n",
    "    \"delfina\",\"daniela\",\"daiana\",\"elena\",\"eliana\",\"emilia\",\"emily\",\"florencia\",\n",
    "    \"fernanda\",\"gabriela\",\"graciela\",\"guadalupe\",\"gimena\",\"ximena\",\"helena\",\n",
    "    \"ivana\",\"ivonne\",\"jennifer\",\"julieta\",\"jazmín\",\"jazmin\",\"karina\",\"keila\",\n",
    "    \"karen\",\"lucia\",\"lucía\",\"luana\",\"luna\",\"luisa\",\"ludmila\",\"maría\",\"maria\",\n",
    "    \"mariana\",\"morena\",\"marta\",\"melina\",\"milena\",\"nadia\",\"noelia\",\"natalia\",\n",
    "    \"nerina\",\"paola\",\"pamela\",\"patricia\",\"pía\",\"pia\",\"romina\",\"rocío\",\"rocio\",\n",
    "    \"sofia\",\"sofía\",\"sol\",\"serena\",\"tamara\",\"tatiana\",\"ursula\",\"vanesa\",\n",
    "    \"vanessa\",\"valeria\",\"valentina\",\"violeta\",\"wendy\",\"xiomara\",\"yesica\",\n",
    "    \"yésica\",\"yanina\",\"zaira\",\"zoe\",\"zoé\",\n",
    "\n",
    "    # Apellidos argentinos frecuentes\n",
    "    \"gonzalez\",\"gonzález\",\"rodriguez\",\"rodríguez\",\"fernandez\",\"fernández\",\n",
    "    \"lopez\",\"lópez\",\"martinez\",\"martínez\",\"garcia\",\"garcía\",\"perez\",\"pérez\",\n",
    "    \"sanchez\",\"sánchez\",\"romero\",\"diaz\",\"díaz\",\"pereyra\",\"pereira\",\"ruiz\",\n",
    "    \"torres\",\"flores\",\"acosta\",\"benitez\",\"benítez\",\"medina\",\"herrera\",\n",
    "    \"castro\",\"nuñez\",\"núñez\",\"ramos\",\"dominguez\",\"domínguez\",\"ortiz\",\n",
    "    \"gimenez\",\"giménez\",\"molina\",\"silva\",\"rios\",\"ríos\",\"suarez\",\"suárez\",\n",
    "    \"alvarez\",\"álvarez\",\"aguirre\",\"mendoza\",\"paz\",\"vera\",\"juarez\",\"juárez\",\n",
    "    \"rivas\",\"gonzaga\",\"montoya\",\"castillo\",\"campos\",\"morales\",\"vargas\",\n",
    "    \"lujan\",\"luján\",\"arias\",\"frias\",\"frías\",\"toledo\",\"solis\",\"solís\",\"moyano\",\n",
    "    \"correa\",\"pineda\",\"cabrera\",\"vazquez\",\"váquez\",\"navarro\",\"rosales\",\n",
    "    \"espinoza\",\"ospina\",\"manrique\",\"salazar\",\n",
    "\n",
    "    # Apellidos hispanos muy frecuentes\n",
    "    \"moreno\",\"rubio\",\"blanco\",\"marquez\",\"márquez\",\"ibarra\",\"salinas\",\"mejia\",\n",
    "    \"ortega\",\"valdez\",\"valdés\",\"caballero\",\"mercedes\",\"ferrer\",\"costas\",\n",
    "    \"robles\",\"delgado\",\"rios\",\"montes\",\"cortez\",\"cortes\",\"carvajal\",\"solano\",\n",
    "    \"pacheco\",\"maldonado\",\"araujo\",\"padilla\",\"velazquez\",\"velázquez\",\n",
    "    \"contreras\",\"sandoval\",\"cordero\",\"miranda\",\"carmona\",\"vidal\",\"rendon\",\n",
    "    \"rendón\",\"villalba\",\"villalobos\",\"arrieta\",\n",
    "\n",
    "    # Casos especiales útiles\n",
    "    \"messi\",\"maradona\",\"riquelme\",\"tevez\",\"di maria\",\"dimaria\",\n",
    "    \"alberto\",\"cristina\",\"milei\",\"macri\"\n",
    "    \n",
    "    # Literatura clásica\n",
    "    \"sherlock\", \"holmes\", \"watson\", \"gatsby\", \"frankenstein\", \"dracula\",\n",
    "    \"harker\", \"van helsing\", \"hyde\", \"jekyll\", \"albus\", \"dumbledore\",\n",
    "    \"frodo\", \"samwise\", \"sam\", \"gandalf\", \"aragorn\", \"boromir\", \"legolas\",\n",
    "    \"bilbo\",\n",
    "\n",
    "    # Harry Potter\n",
    "    \"harry\", \"potter\", \"hermione\", \"granger\", \"ron\", \"weasley\", \"malfoy\",\n",
    "    \"draco\", \"snape\", \"voldemort\", \"sirius\", \"black\", \"hagrid\", \"minerva\",\n",
    "    \"mcgonagall\", \"luna\", \"lovegood\", \"neville\", \"longbottom\",\n",
    "\n",
    "    # Star Wars\n",
    "    \"luke\", \"skywalker\", \"anakin\", \"vader\", \"darth\", \"leia\", \"organa\",\n",
    "    \"han\", \"solo\", \"yoda\", \"kenobi\", \"obi-wan\", \"palpatine\", \"rey\",\n",
    "    \"finn\", \"poe\", \"dameron\", \"chewbacca\",\n",
    "\n",
    "    # Marvel\n",
    "    \"tony\", \"stark\", \"ironman\", \"steve\", \"rogers\", \"thor\", \"loki\",\n",
    "    \"natasha\", \"romanoff\", \"clint\", \"barton\", \"hulk\", \"banner\",\n",
    "    \"peter\", \"parker\", \"spiderman\", \"wanda\", \"maximoff\", \"vision\",\n",
    "    \"tchalla\", \"pantera\", \"negra\",\n",
    "\n",
    "    # DC Comics\n",
    "    \"bruce\", \"wayne\", \"batman\", \"alfred\", \"pennyworth\", \"clark\", \"kent\",\n",
    "    \"superman\", \"lois\", \"lane\", \"diana\", \"prince\", \"wonderwoman\",\n",
    "    \"flash\", \"barry\", \"allen\", \"joker\", \"harley\", \"quinn\",\n",
    "\n",
    "    # Videojuegos\n",
    "    \"mario\", \"luigi\", \"peach\", \"bowser\", \"link\", \"zelda\", \"ganondorf\",\n",
    "    \"samus\", \"master\", \"chief\", \"kratos\", \"atreus\", \"cloud\", \"sephiroth\",\n",
    "    \"sonic\", \"tails\",\n",
    "\n",
    "    # Series / TV\n",
    "    \"walter\", \"white\", \"heisenberg\", \"jesse\", \"pinkman\", \"saul\", \"goodman\",\n",
    "    \"rick\", \"grimes\", \"daryl\", \"dixon\", \"eleven\", \"hopper\", \"mike\",\n",
    "    \"dustin\", \"lucas\", \"nancy\", \"jonathan\", \"vecna\",\n",
    "\n",
    "    # Simpsons\n",
    "    \"homero\", \"homer\", \"marge\", \"bart\", \"lisa\", \"maggie\", \"moe\",\n",
    "    \"burns\", \"smithers\", \"flanders\", \"ned\", \"milhouse\",\n",
    "\n",
    "    # Futurama\n",
    "    \"fry\", \"bender\", \"leela\", \"zoidberg\", \"hermes\", \"professor\",\n",
    "    \"farnsworth\",\n",
    "\n",
    "    # Toy Story\n",
    "    \"woody\", \"buzz\", \"lightyear\", \"bo\", \"peep\", \"jesse\",\n",
    "\n",
    "    # Pixar/Disney\n",
    "    \"nemo\", \"dory\", \"marlin\", \"sully\", \"mike wazowski\", \"boo\",\n",
    "    \"mr incredible\", \"elastigirl\", \"dash\", \"violet\",\n",
    "\n",
    "    # Autos (Cars)\n",
    "    \"rayo\", \"mcqueen\", \"mate\", \"sally\", \"doc\", \"hudson\",\n",
    "\n",
    "    # Shrek\n",
    "    \"shrek\", \"fiona\", \"burro\", \"asno\", \"lord farquaad\", \"jengi\",\n",
    "\n",
    "    # Otros animados\n",
    "    \"goku\", \"vegeta\", \"gohan\", \"piccolo\", \"bulma\", \"trunks\",\n",
    "    \"naruto\", \"sasuke\", \"sakura\", \"kakashi\",\n",
    "\n",
    "    # Anime generales\n",
    "    \"light\", \"yagami\", \"l\", \"lawliet\", \"eren\", \"yeager\", \"mikasa\",\n",
    "    \"armin\", \"levi\",\n",
    "\n",
    "    # Cultura pop general\n",
    "    \"indiana\", \"jones\", \"terminator\", \"neo\", \"trinity\", \"morpheus\",\n",
    "    \"john\", \"wick\", \"yennefer\", \"geralt\", \"ciri\", \"bateman\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el DF en Chunks (evita MemoryError)\n",
    "\n",
    "def procesar_en_chunks(df, tamaño_chunk_instancias=300, nombres_propios=None):\n",
    "\n",
    "    instancias = df[\"instancia_id\"].unique()\n",
    "    chunks = [instancias[i:i+tamaño_chunk_instancias]\n",
    "              for i in range(0, len(instancias), tamaño_chunk_instancias)]\n",
    "\n",
    "    dfs_resultados = []\n",
    "\n",
    "    for i, subset in enumerate(chunks):\n",
    "        print(f\"Procesando chunk {i+1}/{len(chunks)} ({len(subset)} instancias)...\")\n",
    "\n",
    "        df_chunk = df[df[\"instancia_id\"].isin(subset)].copy()\n",
    "\n",
    "        df_proc = crear_features(df_chunk, nombres_propios=nombres_propios)\n",
    "\n",
    "        dfs_resultados.append(df_proc)\n",
    "\n",
    "        del df_chunk\n",
    "        del df_proc\n",
    "        gc.collect()\n",
    "\n",
    "    return pd.concat(dfs_resultados, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporatmos el dataset y creamos una copia en caso de emergencia\n",
    "df_original = pd.read_parquet(\"datos_modelo.parquet\")\n",
    "df = df_original.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir palabras\n",
    "df = reconstruir_palabras(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instancia_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "      <th>punt_inicial</th>\n",
       "      <th>capitalizacion</th>\n",
       "      <th>pfinal</th>\n",
       "      <th>palabra_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>10110</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>##cié</td>\n",
       "      <td>93977</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>##rra</td>\n",
       "      <td>21084</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>##lo</td>\n",
       "      <td>10715</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>10110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>50</td>\n",
       "      <td>##s</td>\n",
       "      <td>10107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>50</td>\n",
       "      <td>está</td>\n",
       "      <td>11559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>50</td>\n",
       "      <td>a</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>50</td>\n",
       "      <td>tu</td>\n",
       "      <td>13055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>50</td>\n",
       "      <td>cargo</td>\n",
       "      <td>15856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1345 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      instancia_id  token  token_id  punt_inicial  capitalizacion  pfinal  \\\n",
       "0                1     en     10110             0               1       0   \n",
       "1                1  ##cié     93977             0               1       0   \n",
       "2                1  ##rra     21084             0               1       0   \n",
       "3                1   ##lo     10715             0               1       0   \n",
       "4                1     en     10110             0               0       0   \n",
       "...            ...    ...       ...           ...             ...     ...   \n",
       "1340            50    ##s     10107             0               0       1   \n",
       "1341            50   está     11559             0               1       0   \n",
       "1342            50      a       169             0               0       0   \n",
       "1343            50     tu     13055             0               0       0   \n",
       "1344            50  cargo     15856             0               0       2   \n",
       "\n",
       "      palabra_id  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              2  \n",
       "...          ...  \n",
       "1340          78  \n",
       "1341          79  \n",
       "1342          80  \n",
       "1343          81  \n",
       "1344          82  \n",
       "\n",
       "[1345 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar en chunks para calcular features sin MemoryError\n",
    "df = procesar_en_chunks(df, tamaño_chunk_instancias=300, nombres_propios= nombres_y_apellidos)\n",
    "\n",
    "# Guardar el resultado para entrenar después sin recalcular embeddings\n",
    "# df.to_parquet(\"datos_modelo_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crear_features_token(df)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"indice\"] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeo\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separo en train y test\n",
    "\n",
    "# obtener lista de instancias\n",
    "ids = df[\"instancia_id\"].unique()\n",
    "\n",
    "train_ids, eval_ids = train_test_split(ids, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train = df[df[\"instancia_id\"].isin(train_ids)].copy()\n",
    "df_eval  = df[df[\"instancia_id\"].isin(eval_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos features\n",
    "FEATURES_TOKEN = [\n",
    "    \"posicion_palabra\",\n",
    "    \"dist_al_final\",\n",
    "    \"longitud_palabra\",\n",
    "    \"frecuencia_en_instancia\",\n",
    "    \"es_nombre\",\n",
    "    \"similitud_con_anterior\",\n",
    "    \"similitud_con_siguiente\",\n",
    "    \"is_subtoken\",\n",
    "    \"token_idx_en_palabra\",\n",
    "    \"token_pos_relativa\",\n",
    "    \"longitud_token\",\n",
    "    \"es_primer_subtoken\",\n",
    "    \"es_ultimo_subtoken\",\n",
    "    \"palabra_anterior_es_nombre\",\n",
    "    \"palabra_siguiente_es_nombre\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cap = df_train[FEATURES_TOKEN].fillna(0)\n",
    "X_eval_cap  = df_eval[FEATURES_TOKEN].fillna(0)\n",
    "\n",
    "y_train_cap = df_train[\"capitalizacion\"].astype(int)\n",
    "y_eval_cap = df_eval[\"capitalizacion\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el RF de Capitalizacion\n",
    "rf_cap = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None) # si lo pasamos a colab quizas ocnviene meter n_jobs=-1\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = cross_val_predict(rf_cap, X_train_cap, y_train_cap, cv=cv, method=\"predict\")\n",
    "df_train[\"pred_cap_oof\"] = oof_preds\n",
    "\n",
    "rf_cap.fit(X_train_cap, y_train_cap)\n",
    "\n",
    "df_eval[\"pred_cap\"] = rf_cap.predict(X_eval_cap)\n",
    "\n",
    "df.loc[df_train[\"indice\"], \"pred_cap\"] = df_train[\"pred_cap_oof\"].values\n",
    "df.loc[df_eval[\"indice\"], \"pred_cap\"] = df_eval[\"pred_cap\"].values\n",
    "\n",
    "print(\"Modelo RF entrenado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemso las predicciones\n",
    "# df[\"pred_cap\"] = rf_cap.predict(df[FEATURES_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion\n",
    "print(\"Capitalizar:\")\n",
    "print(classification_report(y_eval_cap, rf_cap.predict(X_eval_cap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar RFs para puntuacion inicial y final incluyendo pred_cap como feature\n",
    "\n",
    "# añadir pred_cap a FEATURES_TOKEN si no está\n",
    "if \"pred_cap\" not in FEATURES_TOKEN:\n",
    "    FEATURES_TOKEN.append(\"pred_cap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF puntuacion inicial\n",
    "X_train_pini = df_train[FEATURES_TOKEN].fillna(0)\n",
    "y_train_pini = df_train[\"punt_inicial\"].astype(int)\n",
    "\n",
    "\n",
    "X_eval_pini  = df_eval[FEATURES_TOKEN].fillna(0)\n",
    "y_eval_pini = df_eval[\"punt_inicial\"].astype(int)\n",
    "\n",
    "rf_pini = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None)\n",
    "rf_pini.fit(X_train_pini, y_train_pini)\n",
    "\n",
    "df[\"pred_pini\"] = rf_pini.predict(df[FEATURES_TOKEN])\n",
    "\n",
    "# Evaluacion\n",
    "print(\"Puntuacion Inicial:\")\n",
    "print(classification_report(y_eval_pini, rf_pini.predict(X_eval_pini)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF puntuacion final\n",
    "X_train_pfin = df_train[FEATURES_TOKEN].fillna(0)\n",
    "y_train_pfin = df_train[\"pfinal\"].astype(int)\n",
    "\n",
    "X_eval_pfin  = df_eval[FEATURES_TOKEN].fillna(0)\n",
    "y_eval_pfin = df_eval[\"pfinal\"].astype(int)\n",
    "\n",
    "\n",
    "rf_pfin = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None)\n",
    "rf_pfin.fit(X_train_pfin, y_train_pfin)\n",
    "\n",
    "df[\"pred_pfin\"] = rf_pfin.predict(df[FEATURES_TOKEN])\n",
    "\n",
    "\n",
    "print(\"Puntuacion Final:\")\n",
    "print(classification_report(y_eval_pfin, rf_pfin.predict(X_eval_pfin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chekeo final\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
