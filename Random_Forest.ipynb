{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import gc\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar tokenizer y embeddings\n",
    "\n",
    "modelo_bert  = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(modelo_bert)\n",
    "modelo  = BertModel.from_pretrained(modelo_bert)\n",
    "modelo .eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir palabras a partir de subtokens\n",
    "\n",
    "def reconstruir_palabras(df):\n",
    "    \"\"\"\n",
    "    Recibe el df de tokens y devuelve un df con una columna 'palabra_id'\n",
    "    que identifica a qué palabra pertenece cada token.\n",
    "    \"\"\"\n",
    "    \n",
    "    ids_palabra = []\n",
    "    id_actual  = 0\n",
    "\n",
    "    for id_instancia, grupo in df.groupby(\"instancia_id\"):\n",
    "        ids = []\n",
    "        id_actual = 0\n",
    "\n",
    "        for i, tok in enumerate(grupo[\"token\"]):\n",
    "            if tok.startswith(\"##\"):\n",
    "                # mismo palabra_id que el anterior\n",
    "                ids.append(id_actual)\n",
    "            else:\n",
    "                # empieza palabra nueva\n",
    "                id_actual += 1\n",
    "                ids.append(id_actual)\n",
    "\n",
    "        ids_palabra.extend(ids)\n",
    "\n",
    "    df[\"palabra_id\"] = ids_palabra\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir features a nivel palabra\n",
    "\n",
    "def crear_features(df, nombres_propios=None):\n",
    "    \"\"\"\n",
    "    Crea features por palabra y luego las copia por token.\n",
    "    nombres_propios: conjunto con palabras consideradas nombres propios (opcional)\n",
    "    \"\"\"\n",
    "\n",
    "    if nombres_propios is None:\n",
    "        nombres_propios = set()  # si no tenés nada todavía\n",
    "\n",
    "    # reconstruir palabra concatenando subtokens\n",
    "    def unir_subtokens(grupo):\n",
    "        palabra  = \"\"\n",
    "        for tok in grupo[\"token\"].tolist():\n",
    "            if tok.startswith(\"##\"):\n",
    "                palabra += tok[2:]\n",
    "            else:\n",
    "                palabra += tok\n",
    "        return palabra\n",
    "\n",
    "    palabras = df.groupby([\"instancia_id\", \"palabra_id\"]).apply(unir_subtokens).reset_index()\n",
    "    palabras.columns = [\"instancia_id\", \"palabra_id\", \"palabra\"]\n",
    "    df = df.merge(palabras, on=[\"instancia_id\", \"palabra_id\"], how=\"left\")\n",
    "\n",
    "    # Posición en el texto (por palabra)\n",
    "    df[\"posicion_palabra\"] = df.groupby(\"instancia_id\")[\"palabra_id\"].transform(\n",
    "        lambda x: x.rank(method=\"dense\").astype(int)\n",
    "    )\n",
    "\n",
    "    # Cantidad_palabras - posición\n",
    "    df[\"total_palabras\"] = df.groupby(\"instancia_id\")[\"palabra_id\"].transform(\"max\")\n",
    "    df[\"dist_al_final\"] = df[\"total_palabras\"] - df[\"posicion_palabra\"]\n",
    "\n",
    "    # Longitud de la palabra\n",
    "    df[\"longitud_palabra\"] = df[\"palabra\"].str.len()\n",
    "\n",
    "    # Frecuencia de la palabra en la instancia\n",
    "    freq = df.groupby([\"instancia_id\", \"palabra\"])[\"palabra\"].transform(\"count\")\n",
    "    df[\"frecuencia_en_instancia\"] = freq\n",
    "\n",
    "    # es_nombre?\n",
    "    df[\"es_nombre\"] = df[\"palabra\"].isin(nombres_propios).astype(int)\n",
    "\n",
    " \n",
    "    # Embeddings a nivel palabra\n",
    "    \n",
    "    def obtener_embedding_subtoken(token_id):\n",
    "        \"\"\"Embedding del subtoken.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            return modelo.embeddings.word_embeddings.weight[token_id].cpu().numpy()\n",
    "\n",
    "    # obtener embedding por subtoken\n",
    "    df[\"embedding_subtoken\"] = df[\"token_id\"].apply(obtener_embedding_subtoken)\n",
    "\n",
    "    # promediar embeddings de subtokens por palabra\n",
    "    embeddings_palabra  = (\n",
    "        df.groupby([\"instancia_id\", \"palabra_id\"])[\"embedding_subtoken\"]\n",
    "        .apply(lambda xs: np.mean(np.vstack(xs.values), axis=0))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    embeddings_palabra.columns = [\"instancia_id\", \"palabra_id\", \"embedding_palabra\"]\n",
    "\n",
    "    df = df.merge(embeddings_palabra, on=[\"instancia_id\", \"palabra_id\"], how=\"left\")\n",
    "\n",
    "    # eliminar columna auxiliar\n",
    "    df.drop(columns=[\"embedding_subtoken\"], inplace=True)\n",
    "\n",
    "    # Distancia coseno a palabra anterior y siguiente\n",
    "    def similitud_coseno(a, b):\n",
    "        a = torch.tensor(a)\n",
    "        b = torch.tensor(b)\n",
    "        return F.cosine_similarity(a, b, dim=0).item()\n",
    "\n",
    "    df[\"embedding_anterior\"] = df.groupby(\"instancia_id\")[\"embedding_palabra\"].shift(1)\n",
    "    df[\"embedding_siguiente\"] = df.groupby(\"instancia_id\")[\"embedding_palabra\"].shift(-1)\n",
    "\n",
    "    def calcular_similitud(fila, columna):\n",
    "        valor = fila[columna]\n",
    "        if valor is None or (isinstance(valor, float) and np.isnan(valor)):\n",
    "            return 0\n",
    "        return similitud_coseno(fila[\"embedding_palabra\"], fila[columna])\n",
    "\n",
    "    df[\"similitud_con_anterior\"] = df.apply(lambda r: calcular_similitud(r, \"embedding_anterior\"), axis=1)\n",
    "    df[\"similitud_con_siguiente\"] = df.apply(lambda r: calcular_similitud(r, \"embedding_siguiente\"), axis=1)\n",
    "\n",
    "    # Quitar embeddings auxiliares\n",
    "    df.drop(columns=[\"embedding_anterior\", \"embedding_siguiente\"], inplace=True)\n",
    "    df.drop(columns=[\"embedding_palabra\"], inplace=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Crear features a nivel token (fusionando features palabra)\n",
    "\n",
    "def crear_features_token(df):\n",
    "    \"\"\"\n",
    "    Recibe df que ya tiene las columnas por token + las columnas\n",
    "    a nivel palabra (palabra, posicion_palabra, dist_al_final, longitud_palabra,\n",
    "    frecuencia_en_instancia, es_nombre, similitud_con_anterior, similitud_con_siguiente (opc))\n",
    "    Devuelve df_tokens con features listas para RF de puntuacion (por token).\n",
    "    \"\"\"\n",
    "\n",
    "    # -- Copiamos para no mutar original --\n",
    "    df_tok = df.copy()\n",
    "\n",
    "    # -- marcar subtokens --\n",
    "    df_tok[\"is_subtoken\"] = df_tok[\"token\"].str.startswith(\"##\").astype(int)\n",
    "\n",
    "    # índice del token dentro de la palabra (1 = primer subtoken)\n",
    "    def idx_en_palabra(gr):\n",
    "        # devuelve índice por el orden de aparición en el grupo\n",
    "        return np.arange(1, len(gr) + 1)\n",
    "\n",
    "    df_tok[\"token_idx_en_palabra\"] = df_tok.groupby(\n",
    "        [\"instancia_id\", \"palabra_id\"]).cumcount() + 1\n",
    "\n",
    "    # tamaño (longitud) del token textual (sin ##)\n",
    "    df_tok[\"token_text\"] = df_tok[\"token\"].str.replace(\"^##\", \"\", regex=True)\n",
    "    df_tok[\"longitud_token\"] = df_tok[\"token_text\"].str.len()\n",
    "\n",
    "    # es primer subtoken de la palabra?\n",
    "    df_tok[\"es_primer_subtoken\"] = (df_tok[\"token_idx_en_palabra\"] == 1).astype(int)\n",
    "\n",
    "    # es ultimo subtoken\n",
    "    subtoks_por_palabra = df_tok.groupby([\"instancia_id\", \"palabra_id\"])[\"token\"].transform(\"count\")\n",
    "    df_tok[\"es_ultimo_subtoken\"] = (df_tok[\"token_idx_en_palabra\"] == subtoks_por_palabra).astype(int)\n",
    "\n",
    "    # indicar si la palabra anterior / siguiente es nombre propio (útil para puntuación)\n",
    "    df_tok[\"palabra_anterior_es_nombre\"] = df_tok.groupby(\"instancia_id\")[\"es_nombre\"].shift(1).fillna(0).astype(int)\n",
    "    df_tok[\"palabra_siguiente_es_nombre\"] = df_tok.groupby(\"instancia_id\")[\"es_nombre\"].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "    # Distancia en caracteres al inicio/fin del token dentro de la palabra\n",
    "    df_tok[\"token_pos_relativa\"] = df_tok[\"token_idx_en_palabra\"] / subtoks_por_palabra\n",
    "\n",
    "    # Eliminar columnas intermedias\n",
    "    df_tok.drop(columns=[\"token_text\"], inplace=True)\n",
    "\n",
    "    return df_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de nombres y apellidos\n",
    "\n",
    "nombres_y_apellidos = {\n",
    "\n",
    "    # Nombres masculinos\n",
    "    \"juan\",\"jose\",\"josé\",\"javier\",\"jorge\",\"julian\",\"julían\",\"julio\",\"joel\",\n",
    "    \"joaquin\",\"joaquín\",\"miguel\",\"martin\",\"martín\",\"marco\",\"marcos\",\"mateo\",\n",
    "    \"matias\",\"matías\",\"maximiliano\",\"manuel\",\"mariano\",\"mauricio\",\"mirko\",\n",
    "    \"nahuel\",\"nicolas\",\"nicólas\",\"nazareno\",\"pablo\",\"pedro\",\"patricio\",\n",
    "    \"ramiro\",\"ricardo\",\"roberto\",\"rodrigo\",\"román\",\"santiago\",\"sergio\",\n",
    "    \"sebastian\",\"sebastián\",\"samuel\",\"tomás\",\"tomas\",\"thiago\",\"tiago\",\n",
    "    \"ulises\",\"victor\",\"víctor\",\"valentin\",\"valentín\",\"william\",\"walter\",\n",
    "    \"xavier\",\"yago\",\"yamil\",\"zaid\",\"zair\",\"zahir\",\n",
    "\n",
    "    # Nombres femeninos\n",
    "    \"ana\",\"andrea\",\"antonella\",\"antonela\",\"agustina\",\"belén\",\"belen\",\"brenda\",\n",
    "    \"brisa\",\"bárbara\",\"barbara\",\"camila\",\"celeste\",\"carolina\",\"candela\",\n",
    "    \"delfina\",\"daniela\",\"daiana\",\"elena\",\"eliana\",\"emilia\",\"emily\",\"florencia\",\n",
    "    \"fernanda\",\"gabriela\",\"graciela\",\"guadalupe\",\"gimena\",\"ximena\",\"helena\",\n",
    "    \"ivana\",\"ivonne\",\"jennifer\",\"julieta\",\"jazmín\",\"jazmin\",\"karina\",\"keila\",\n",
    "    \"karen\",\"lucia\",\"lucía\",\"luana\",\"luna\",\"luisa\",\"ludmila\",\"maría\",\"maria\",\n",
    "    \"mariana\",\"morena\",\"marta\",\"melina\",\"milena\",\"nadia\",\"noelia\",\"natalia\",\n",
    "    \"nerina\",\"paola\",\"pamela\",\"patricia\",\"pía\",\"pia\",\"romina\",\"rocío\",\"rocio\",\n",
    "    \"sofia\",\"sofía\",\"sol\",\"serena\",\"tamara\",\"tatiana\",\"ursula\",\"vanesa\",\n",
    "    \"vanessa\",\"valeria\",\"valentina\",\"violeta\",\"wendy\",\"xiomara\",\"yesica\",\n",
    "    \"yésica\",\"yanina\",\"zaira\",\"zoe\",\"zoé\",\n",
    "\n",
    "    # Apellidos argentinos frecuentes\n",
    "    \"gonzalez\",\"gonzález\",\"rodriguez\",\"rodríguez\",\"fernandez\",\"fernández\",\n",
    "    \"lopez\",\"lópez\",\"martinez\",\"martínez\",\"garcia\",\"garcía\",\"perez\",\"pérez\",\n",
    "    \"sanchez\",\"sánchez\",\"romero\",\"diaz\",\"díaz\",\"pereyra\",\"pereira\",\"ruiz\",\n",
    "    \"torres\",\"flores\",\"acosta\",\"benitez\",\"benítez\",\"medina\",\"herrera\",\n",
    "    \"castro\",\"nuñez\",\"núñez\",\"ramos\",\"dominguez\",\"domínguez\",\"ortiz\",\n",
    "    \"gimenez\",\"giménez\",\"molina\",\"silva\",\"rios\",\"ríos\",\"suarez\",\"suárez\",\n",
    "    \"alvarez\",\"álvarez\",\"aguirre\",\"mendoza\",\"paz\",\"vera\",\"juarez\",\"juárez\",\n",
    "    \"rivas\",\"gonzaga\",\"montoya\",\"castillo\",\"campos\",\"morales\",\"vargas\",\n",
    "    \"lujan\",\"luján\",\"arias\",\"frias\",\"frías\",\"toledo\",\"solis\",\"solís\",\"moyano\",\n",
    "    \"correa\",\"pineda\",\"cabrera\",\"vazquez\",\"váquez\",\"navarro\",\"rosales\",\n",
    "    \"espinoza\",\"ospina\",\"manrique\",\"salazar\",\n",
    "\n",
    "    # Apellidos hispanos muy frecuentes\n",
    "    \"moreno\",\"rubio\",\"blanco\",\"marquez\",\"márquez\",\"ibarra\",\"salinas\",\"mejia\",\n",
    "    \"ortega\",\"valdez\",\"valdés\",\"caballero\",\"mercedes\",\"ferrer\",\"costas\",\n",
    "    \"robles\",\"delgado\",\"rios\",\"montes\",\"cortez\",\"cortes\",\"carvajal\",\"solano\",\n",
    "    \"pacheco\",\"maldonado\",\"araujo\",\"padilla\",\"velazquez\",\"velázquez\",\n",
    "    \"contreras\",\"sandoval\",\"cordero\",\"miranda\",\"carmona\",\"vidal\",\"rendon\",\n",
    "    \"rendón\",\"villalba\",\"villalobos\",\"arrieta\",\n",
    "\n",
    "    # Casos especiales útiles\n",
    "    \"messi\",\"maradona\",\"riquelme\",\"tevez\",\"di maria\",\"dimaria\",\n",
    "    \"alberto\",\"cristina\",\"milei\",\"macri\"\n",
    "    \n",
    "    # Literatura clásica\n",
    "    \"sherlock\", \"holmes\", \"watson\", \"gatsby\", \"frankenstein\", \"dracula\",\n",
    "    \"harker\", \"van helsing\", \"hyde\", \"jekyll\", \"albus\", \"dumbledore\",\n",
    "    \"frodo\", \"samwise\", \"sam\", \"gandalf\", \"aragorn\", \"boromir\", \"legolas\",\n",
    "    \"bilbo\",\n",
    "\n",
    "    # Harry Potter\n",
    "    \"harry\", \"potter\", \"hermione\", \"granger\", \"ron\", \"weasley\", \"malfoy\",\n",
    "    \"draco\", \"snape\", \"voldemort\", \"sirius\", \"black\", \"hagrid\", \"minerva\",\n",
    "    \"mcgonagall\", \"luna\", \"lovegood\", \"neville\", \"longbottom\",\n",
    "\n",
    "    # Star Wars\n",
    "    \"luke\", \"skywalker\", \"anakin\", \"vader\", \"darth\", \"leia\", \"organa\",\n",
    "    \"han\", \"solo\", \"yoda\", \"kenobi\", \"obi-wan\", \"palpatine\", \"rey\",\n",
    "    \"finn\", \"poe\", \"dameron\", \"chewbacca\",\n",
    "\n",
    "    # Marvel\n",
    "    \"tony\", \"stark\", \"ironman\", \"steve\", \"rogers\", \"thor\", \"loki\",\n",
    "    \"natasha\", \"romanoff\", \"clint\", \"barton\", \"hulk\", \"banner\",\n",
    "    \"peter\", \"parker\", \"spiderman\", \"wanda\", \"maximoff\", \"vision\",\n",
    "    \"tchalla\", \"pantera\", \"negra\",\n",
    "\n",
    "    # DC Comics\n",
    "    \"bruce\", \"wayne\", \"batman\", \"alfred\", \"pennyworth\", \"clark\", \"kent\",\n",
    "    \"superman\", \"lois\", \"lane\", \"diana\", \"prince\", \"wonderwoman\",\n",
    "    \"flash\", \"barry\", \"allen\", \"joker\", \"harley\", \"quinn\",\n",
    "\n",
    "    # Videojuegos\n",
    "    \"mario\", \"luigi\", \"peach\", \"bowser\", \"link\", \"zelda\", \"ganondorf\",\n",
    "    \"samus\", \"master\", \"chief\", \"kratos\", \"atreus\", \"cloud\", \"sephiroth\",\n",
    "    \"sonic\", \"tails\",\n",
    "\n",
    "    # Series / TV\n",
    "    \"walter\", \"white\", \"heisenberg\", \"jesse\", \"pinkman\", \"saul\", \"goodman\",\n",
    "    \"rick\", \"grimes\", \"daryl\", \"dixon\", \"eleven\", \"hopper\", \"mike\",\n",
    "    \"dustin\", \"lucas\", \"nancy\", \"jonathan\", \"vecna\",\n",
    "\n",
    "    # Simpsons\n",
    "    \"homero\", \"homer\", \"marge\", \"bart\", \"lisa\", \"maggie\", \"moe\",\n",
    "    \"burns\", \"smithers\", \"flanders\", \"ned\", \"milhouse\",\n",
    "\n",
    "    # Futurama\n",
    "    \"fry\", \"bender\", \"leela\", \"zoidberg\", \"hermes\", \"professor\",\n",
    "    \"farnsworth\",\n",
    "\n",
    "    # Toy Story\n",
    "    \"woody\", \"buzz\", \"lightyear\", \"bo\", \"peep\", \"jesse\",\n",
    "\n",
    "    # Pixar/Disney\n",
    "    \"nemo\", \"dory\", \"marlin\", \"sully\", \"mike wazowski\", \"boo\",\n",
    "    \"mr incredible\", \"elastigirl\", \"dash\", \"violet\",\n",
    "\n",
    "    # Autos (Cars)\n",
    "    \"rayo\", \"mcqueen\", \"mate\", \"sally\", \"doc\", \"hudson\",\n",
    "\n",
    "    # Shrek\n",
    "    \"shrek\", \"fiona\", \"burro\", \"asno\", \"lord farquaad\", \"jengi\",\n",
    "\n",
    "    # Otros animados\n",
    "    \"goku\", \"vegeta\", \"gohan\", \"piccolo\", \"bulma\", \"trunks\",\n",
    "    \"naruto\", \"sasuke\", \"sakura\", \"kakashi\",\n",
    "\n",
    "    # Anime generales\n",
    "    \"light\", \"yagami\", \"l\", \"lawliet\", \"eren\", \"yeager\", \"mikasa\",\n",
    "    \"armin\", \"levi\",\n",
    "\n",
    "    # Cultura pop general\n",
    "    \"indiana\", \"jones\", \"terminator\", \"neo\", \"trinity\", \"morpheus\",\n",
    "    \"john\", \"wick\", \"yennefer\", \"geralt\", \"ciri\", \"bateman\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el DF en Chunks (evita MemoryError)\n",
    "\n",
    "def procesar_en_chunks(df, tamaño_chunk_instancias=300, nombres_propios=None):\n",
    "\n",
    "    instancias = df[\"instancia_id\"].unique()\n",
    "    chunks = [instancias[i:i+tamaño_chunk_instancias]\n",
    "              for i in range(0, len(instancias), tamaño_chunk_instancias)]\n",
    "\n",
    "    dfs_resultados = []\n",
    "\n",
    "    for i, subset in enumerate(chunks):\n",
    "        print(f\"Procesando chunk {i+1}/{len(chunks)} ({len(subset)} instancias)...\")\n",
    "\n",
    "        df_chunk = df[df[\"instancia_id\"].isin(subset)].copy()\n",
    "\n",
    "        df_proc = crear_features(df_chunk, nombres_propios=nombres_propios)\n",
    "\n",
    "        dfs_resultados.append(df_proc)\n",
    "\n",
    "        del df_chunk\n",
    "        del df_proc\n",
    "        gc.collect()\n",
    "\n",
    "    return pd.concat(dfs_resultados, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de los Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporatmos el dataset y creamos una copia en caso de emergencia\n",
    "df_original = pd.read_parquet(\"datos_entrenamiento_RF_60000.parquet\")\n",
    "df = df_original.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir palabras\n",
    "df = reconstruir_palabras(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar en chunks para calcular features sin MemoryError\n",
    "df = procesar_en_chunks(df, tamaño_chunk_instancias=300, nombres_propios= nombres_y_apellidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el DF para calcular features a nivel token\n",
    "df = crear_features_token(df)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"indice\"] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separo en train y test\n",
    "\n",
    "# obtener lista de instancias\n",
    "ids = df[\"instancia_id\"].unique()\n",
    "\n",
    "train_ids, eval_ids = train_test_split(ids, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train = df[df[\"instancia_id\"].isin(train_ids)].copy()\n",
    "df_eval  = df[df[\"instancia_id\"].isin(eval_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos features\n",
    "FEATURES_TOKEN = [\n",
    "    \"posicion_palabra\",\n",
    "    \"dist_al_final\",\n",
    "    \"longitud_palabra\",\n",
    "    \"frecuencia_en_instancia\",\n",
    "    \"es_nombre\",\n",
    "    \"similitud_con_anterior\",\n",
    "    \"similitud_con_siguiente\",\n",
    "    \"is_subtoken\",\n",
    "    \"token_idx_en_palabra\",\n",
    "    \"token_pos_relativa\",\n",
    "    \"longitud_token\",\n",
    "    \"es_primer_subtoken\",\n",
    "    \"es_ultimo_subtoken\",\n",
    "    \"palabra_anterior_es_nombre\",\n",
    "    \"palabra_siguiente_es_nombre\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cap = df_train[FEATURES_TOKEN].fillna(0)\n",
    "X_eval_cap  = df_eval[FEATURES_TOKEN].fillna(0)\n",
    "\n",
    "y_train_cap = df_train[\"capitalizacion\"].astype(int)\n",
    "y_eval_cap = df_eval[\"capitalizacion\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el RF de Capitalizacion\n",
    "rf_cap = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = cross_val_predict(rf_cap, X_train_cap, y_train_cap, cv=cv, method=\"predict\")\n",
    "df_train[\"pred_cap\"] = oof_preds # pred_cap_oof (esto supuestamente evita leakege porque despues se usan estas predicciones como features para los otros RF\n",
    "                                 # y si calculamos las predicciones para los datos de train, los toros RFs estarian recibiendo información que el primer modelo \n",
    "                                 # no podría producir en un escenario real, )\n",
    "\n",
    "rf_cap.fit(X_train_cap, y_train_cap)\n",
    "\n",
    "df_eval[\"pred_cap\"] = rf_cap.predict(X_eval_cap)\n",
    "\n",
    "df.loc[df_train[\"indice\"], \"pred_cap\"] = df_train[\"pred_cap\"].values\n",
    "df.loc[df_eval[\"indice\"], \"pred_cap\"] = df_eval[\"pred_cap\"].values\n",
    "\n",
    "print(\"Modelo RF entrenado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion\n",
    "print(\"Capitalizar:\")\n",
    "print(classification_report(y_eval_cap, rf_cap.predict(X_eval_cap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo entrenado\n",
    "\n",
    "joblib.dump(rf_cap, \"rf_cap_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos las predicciones del RF anterior como feature para los RFs de puntuacion\n",
    "\n",
    "# añadir pred_cap a FEATURES_TOKEN si no está\n",
    "if \"pred_cap\" not in FEATURES_TOKEN:\n",
    "    FEATURES_TOKEN.append(\"pred_cap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos los datos en Train y Test\n",
    "ids = df[\"instancia_id\"].unique()\n",
    "\n",
    "train_ids, eval_ids = train_test_split(ids, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train = df[df[\"instancia_id\"].isin(train_ids)].copy()\n",
    "df_eval  = df[df[\"instancia_id\"].isin(eval_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF puntuacion inicial\n",
    "X_train_pini = df_train[FEATURES_TOKEN].fillna(0)\n",
    "y_train_pini = df_train[\"punt_inicial\"].astype(int)\n",
    "\n",
    "\n",
    "X_eval_pini  = df_eval[FEATURES_TOKEN].fillna(0)\n",
    "y_eval_pini = df_eval[\"punt_inicial\"].astype(int)\n",
    "\n",
    "print(\"Entrenadno el arborl\")\n",
    "rf_pini = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None)\n",
    "rf_pini.fit(X_train_pini, y_train_pini)\n",
    "\n",
    "df[\"pred_pini\"] = rf_pini.predict(df[FEATURES_TOKEN])\n",
    "\n",
    "# Evaluacion\n",
    "print(\"Puntuacion Inicial:\")\n",
    "print(classification_report(y_eval_pini, rf_pini.predict(X_eval_pini)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_pini, \"rf_pini_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF puntuacion final\n",
    "X_train_pfin = df_train[FEATURES_TOKEN].fillna(0)\n",
    "y_train_pfin = df_train[\"pfinal\"].astype(int)\n",
    "\n",
    "X_eval_pfin  = df_eval[FEATURES_TOKEN].fillna(0)\n",
    "y_eval_pfin = df_eval[\"pfinal\"].astype(int)\n",
    "\n",
    "\n",
    "rf_pfin = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None)\n",
    "rf_pfin.fit(X_train_pfin, y_train_pfin)\n",
    "\n",
    "df[\"pred_pfin\"] = rf_pfin.predict(df[FEATURES_TOKEN])\n",
    "\n",
    "# Evaluacion\n",
    "print(\"Puntuacion Final:\")\n",
    "print(classification_report(y_eval_pfin, rf_pfin.predict(X_eval_pfin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_pfin, \"rf_pfin_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos de Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los RFs ya entrenados\n",
    "rf_cap = joblib.load(\"rf_cap_model.pkl\")\n",
    "rf_pini = joblib.load(\"rf_pini_model.pkl\")\n",
    "rf_pfin = joblib.load(\"rf_pfin_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporatmos el dataset y creamos una copia en caso de emergencia\n",
    "df_original = pd.read_parquet(\"datos_control_100000.parquet\")\n",
    "df = df_original.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir palabras\n",
    "df = reconstruir_palabras(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar en chunks para calcular features sin MemoryError\n",
    "df = procesar_en_chunks(df, tamaño_chunk_instancias=300, nombres_propios= nombres_y_apellidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crear_features_token(df)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos las predicciones para Capitalizacion\n",
    "\n",
    "FEATURES_TOKEN = [\n",
    "    \"posicion_palabra\",\n",
    "    \"dist_al_final\",\n",
    "    \"longitud_palabra\",\n",
    "    \"frecuencia_en_instancia\",\n",
    "    \"es_nombre\",\n",
    "    \"similitud_con_anterior\",\n",
    "    \"similitud_con_siguiente\",\n",
    "    \"is_subtoken\",\n",
    "    \"token_idx_en_palabra\",\n",
    "    \"token_pos_relativa\",\n",
    "    \"longitud_token\",\n",
    "    \"es_primer_subtoken\",\n",
    "    \"es_ultimo_subtoken\",\n",
    "    \"palabra_anterior_es_nombre\",\n",
    "    \"palabra_siguiente_es_nombre\"\n",
    "]\n",
    "\n",
    "df[\"pred_cap\"] = rf_cap.predict(df[FEATURES_TOKEN].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"pred_cap\" not in FEATURES_TOKEN:\n",
    "    FEATURES_TOKEN.append(\"pred_cap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos las predicciones para Puntuacion Inicial\n",
    "df[\"pred_pini\"] = rf_pini.predict(df[FEATURES_TOKEN].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos las predicciones para Puntuacion Final\n",
    "df[\"pred_pfin\"] = rf_pfin.predict(df[FEATURES_TOKEN].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion Capitalizacion\n",
    "\n",
    "print(classification_report(df[\"capitalizacion\"], df[\"pred_cap\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion Capitalizacion\n",
    "\n",
    "print(classification_report(df[\"punt_inicial\"], df[\"pred_pini\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion Capitalizacion\n",
    "\n",
    "print(classification_report(df[\"pfinal\"], df[\"pred_pfin\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
