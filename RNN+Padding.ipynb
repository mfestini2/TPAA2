{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset que devuelva SECUENCIAS\n",
    "# antes cada getitem devolvía UN SOLO TOKEN,\n",
    "# lo cual hace imposible usar padding, batching.\n",
    "# Ahora se agrupaman por instancia_id.\n",
    "\n",
    "class dataset_secuencias(Dataset):\n",
    "    def __init__(self, df, cant_instancias):\n",
    "        # Agrupamos por secuencia\n",
    "        # Cada key = instancia_id\n",
    "        # Cada value = dataframe con todos sus tokens\n",
    "        df = df[df[\"instncia_id\"] <= cant_instancias]\n",
    "        self.groups = df.groupby(\"instancia_id\")\n",
    "        self.keys = list(self.groups.groups.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instancia_id = self.keys[idx]\n",
    "        group = self.groups.get_group(instancia_id)\n",
    "\n",
    "        tokens = torch.tensor(group[\"token_id\"].tolist(), dtype=torch.long)\n",
    "        labels_cap = torch.tensor(group[\"capitalizacion\"].tolist(), dtype=torch.long)\n",
    "        labels_pi = torch.tensor(group[\"punt_inicial\"].tolist(), dtype=torch.long)\n",
    "        labels_pf = torch.tensor(group[\"pfinal\"].tolist(), dtype=torch.long)\n",
    "\n",
    "        return tokens, labels_cap, labels_pi, labels_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn con padding\n",
    "#  DataLoader necesita un modo de juntar\n",
    "#  secuencias de distintos tamaños y\n",
    "#  rellenar con padding para formar un batch.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch es una lista de tuplas: (tokens, cap, pi, pf)\n",
    "\n",
    "    # Ordenamos por longitud (requerido por pack_padded_sequence)\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    seqs = [item[0] for item in batch]\n",
    "    caps = [item[1] for item in batch]\n",
    "    pis = [item[2] for item in batch]\n",
    "    pfs = [item[3] for item in batch]\n",
    "\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    max_len = lengths.max()\n",
    "\n",
    "    # Padding manual\n",
    "    pad_token = 0\n",
    "    pad_label = -100  # Ignorado por CrossEntropyLoss\n",
    "\n",
    "    padded_seqs = torch.full((len(batch), max_len), pad_token, dtype=torch.long)\n",
    "    padded_cap = torch.full((len(batch), max_len), pad_label, dtype=torch.long)\n",
    "    padded_pi = torch.full((len(batch), max_len), pad_label, dtype=torch.long)\n",
    "    padded_pf = torch.full((len(batch), max_len), pad_label, dtype=torch.long)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        L = lengths[i]\n",
    "        padded_seqs[i, :L] = seqs[i]\n",
    "        padded_cap[i, :L] = caps[i]\n",
    "        padded_pi[i, :L] = pis[i]\n",
    "        padded_pf[i, :L] = pfs[i]\n",
    "\n",
    "    return padded_seqs, padded_cap, padded_pi, padded_pf, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacion de la RNN\n",
    "\n",
    "class Uni_RNN(nn.Module):\n",
    "    # hidden1: 256-384 según tu config\n",
    "    # hidden2: 128-256 según tu config\n",
    "    def __init__(self, hidden1=512, hidden2=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.embedding_matrix = bert.embeddings.word_embeddings.weight\n",
    "        self.embedding_dim = self.embedding_matrix.shape[1]\n",
    "\n",
    "        self.LSTM = nn.LSTM(self.embedding_dim, hidden1, batch_first=True)\n",
    "        self.LSTM2 = nn.LSTM(hidden1, hidden2, batch_first=True)\n",
    "\n",
    "        self.fc_cap = nn.Linear(hidden2, 4)   # MULTICLASE\n",
    "        self.fc_pinicial = nn.Linear(hidden2, 2)   # (2 clases)\n",
    "        self.fc_pfinal = nn.Linear(hidden2, 4)   # MULTICLASE\n",
    "\n",
    "    def forward(self, seqs, lengths, inferencia=False):\n",
    "        \n",
    "        emb = self.embedding_matrix[seqs]\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            emb, lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "         \n",
    "        data, _ = self.LSTM(packed)\n",
    "        data, _ = self.LSTM2(data)\n",
    "        \n",
    "        final, _ = nn.utils.rnn.pad_packed_sequence(data, batch_first=True)\n",
    "\n",
    "        output_cap = self.fc_cap(final)\n",
    "        output_punt_inicial = self.fc_pinicial(final)\n",
    "        output_punt_final = self.fc_pfinal(final)\n",
    "\n",
    "        if inferencia:\n",
    "            \n",
    "            output_cap = torch.softmax(output_cap, dim=-1)\n",
    "            output_cap = torch.argmax(output_cap, dim=-1)\n",
    "            \n",
    "            output_punt_inicial = torch.softmax(output_punt_inicial, dim=-1)\n",
    "            output_cap = torch.argmax(output_punt_inicial, dim=-1)\n",
    "            \n",
    "            output_punt_final = torch.softmax(output_punt_final, dim=-1)\n",
    "            output_cap = torch.argmax(output_punt_final, dim=-1)\n",
    "\n",
    "        return output_cap, output_punt_inicial, output_punt_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataframe y crear dataloader\n",
    "df = pd.read_parquet(\"datos_modelo.parquet\")\n",
    "\n",
    "dataset = dataset_secuencias(df)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Modelo\n",
    "model = Uni_RNN(\n",
    "    hidden1=512,\n",
    "    hidden2=256\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "ce  = nn.CrossEntropyLoss(ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "num_epoch=5\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for seqs, caps, pini, pfin, lengths in train_loader:\n",
    "        \n",
    "        \n",
    "        out_cap, out_pini, out_pfin = model(seqs, lengths)\n",
    "        \n",
    "        # Pérdida capitalización (multiclase)\n",
    "        loss_cap = ce(\n",
    "            out_cap.reshape(-1, 4),\n",
    "            caps.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Pérdida puntuación inicial\n",
    "        loss_pini  = ce(\n",
    "            out_pini.reshape(-1, 2),\n",
    "            pfin.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Pérdida puntuación final (multiclase)\n",
    "        loss_pfin = ce(\n",
    "            out_pfin.reshape(-1, 4),\n",
    "            pfin.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        loss = loss_cap + loss_pini + loss_pfin\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for seqs, caps, pini, pfin, lengths in valid_loader:\n",
    "            \n",
    "            \n",
    "            out_cap, out_pini, out_pfin = model(seqs, lengths)\n",
    "            \n",
    "            # capitalización\n",
    "            loss_cap = ce(\n",
    "                out_cap.reshape(-1, 4),\n",
    "                caps.reshape(-1)\n",
    "            )\n",
    "            \n",
    "            # Pérdida puntuación inicial\n",
    "            loss_pini  = ce(\n",
    "            out_pini.reshape(-1, 2),\n",
    "            pfin.reshape(-1)\n",
    "            )\n",
    "            \n",
    "            # puntuación final\n",
    "            loss_pfin = ce(\n",
    "                out_pfin.reshape(-1, 4),\n",
    "                pfin.reshape(-1)\n",
    "            )\n",
    "            \n",
    "    \n",
    "            loss = loss_cap + loss_pini + loss_pfin\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epoch} | Train Loss = {total_loss:.4f} | Val Loss = {val_loss:.4f}\")        \n",
    "    \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
