{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUiPuLJgiwL3",
        "outputId": "73cb73dc-af78-482a-9b03-c8ba5a37dcff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hYwamdFYhO-q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wBZoki83h2aZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ead0355-9f8d-412f-d021-c5159f63d3fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5C331293hO-s"
      },
      "outputs": [],
      "source": [
        "# Dataset que devuelva SECUENCIAS\n",
        "# antes cada getitem devolvía UN SOLO TOKEN,\n",
        "# lo cual hace imposible usar padding, batching.\n",
        "# Ahora se agrupaman por instancia_id.\n",
        "\n",
        "class dataset_secuencias(Dataset):\n",
        "    def __init__(self, df,cant_instancias=None):\n",
        "\n",
        "        # Agrupamos por secuencia\n",
        "        # Cada key = instancia_id\n",
        "        # Cada value = dataframe con todos sus tokens\n",
        "\n",
        "        if cant_instancias != None:\n",
        "          df=df[df[\"instancia_id\"]<=cant_instancias]\n",
        "\n",
        "        self.groups = df.groupby(\"instancia_id\")\n",
        "        self.keys = list(self.groups.groups.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        instancia_id = self.keys[idx]\n",
        "        group = self.groups.get_group(instancia_id)\n",
        "\n",
        "        tokens = torch.tensor(group[\"token_id\"].tolist(), dtype=torch.long)\n",
        "        labels_cap = torch.tensor(group[\"capitalizacion\"].tolist(), dtype=torch.long)\n",
        "        labels_pi = torch.tensor(group[\"punt_inicial\"].tolist(), dtype=torch.long)\n",
        "        labels_pf = torch.tensor(group[\"pfinal\"].tolist(), dtype=torch.long)\n",
        "\n",
        "        return tokens, labels_cap, labels_pi, labels_pf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ndp9x_7MhO-u"
      },
      "outputs": [],
      "source": [
        "# collate_fn con padding\n",
        "#  DataLoader necesita un modo de juntar\n",
        "#  secuencias de distintos tamaños y\n",
        "#  rellenar con padding para formar un batch.\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # batch es una lista de tuplas: (tokens, cap, pi, pf)\n",
        "\n",
        "    # Ordenamos por longitud (requerido por pack_padded_sequence)\n",
        "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "\n",
        "    seqs = [item[0] for item in batch]\n",
        "    caps = [item[1] for item in batch]\n",
        "    pis = [item[2] for item in batch]\n",
        "    pfs = [item[3] for item in batch]\n",
        "\n",
        "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "    max_len = lengths.max()\n",
        "\n",
        "    # Padding manual\n",
        "    pad_token = 0\n",
        "    pad_label = -100  # Ignorado por CrossEntropyLoss\n",
        "\n",
        "    padded_seqs = torch.full((len(batch), max_len), pad_token, dtype=torch.long)\n",
        "    padded_cap = torch.full((len(batch), max_len), pad_label, dtype=torch.long)\n",
        "    padded_pi = torch.full((len(batch), max_len), pad_label, dtype=torch.long)\n",
        "    padded_pf = torch.full((len(batch), max_len), pad_label, dtype=torch.long)\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "        L = lengths[i]\n",
        "        padded_seqs[i, :L] = seqs[i]\n",
        "        padded_cap[i, :L] = caps[i]\n",
        "        padded_pi[i, :L] = pis[i]\n",
        "        padded_pf[i, :L] = pfs[i]\n",
        "\n",
        "    return padded_seqs, padded_cap, padded_pi, padded_pf, lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1Mwd3epThO-v"
      },
      "outputs": [],
      "source": [
        "# Implementacion de la RNN\n",
        "\n",
        "class Uni_RNN(nn.Module):\n",
        "    # hidden1: 256-384 según tu config\n",
        "    # hidden2: 128-256 según tu config\n",
        "    def __init__(self, hidden1=512, hidden2=256):\n",
        "        super().__init__()\n",
        "\n",
        "        bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        self.embedding_matrix = bert.embeddings.word_embeddings.weight\n",
        "        self.embedding_dim = self.embedding_matrix.shape[1]\n",
        "\n",
        "        self.LSTM = nn.LSTM(self.embedding_dim, hidden1, batch_first=True)\n",
        "        self.LSTM2 = nn.LSTM(hidden1, hidden2, batch_first=True)\n",
        "\n",
        "        self.fc_cap = nn.Linear(hidden2, 4)   # MULTICLASE\n",
        "        self.fc_pinicial = nn.Linear(hidden2, 2)   # (2 clases)\n",
        "        self.fc_pfinal = nn.Linear(hidden2, 4)   # MULTICLASE\n",
        "\n",
        "    def forward(self, seqs, lengths, inferencia=False):\n",
        "\n",
        "        emb = self.embedding_matrix[seqs]\n",
        "\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            emb, lengths.cpu(), batch_first=True, enforce_sorted=True\n",
        "        )\n",
        "\n",
        "        data, _ = self.LSTM(packed)\n",
        "        data, _ = self.LSTM2(data)\n",
        "\n",
        "        final, _ = nn.utils.rnn.pad_packed_sequence(data, batch_first=True)\n",
        "\n",
        "        output_cap = self.fc_cap(final)\n",
        "        output_punt_inicial = self.fc_pinicial(final)\n",
        "        output_punt_final = self.fc_pfinal(final)\n",
        "\n",
        "        if inferencia:\n",
        "            output_cap = torch.softmax(output_cap, dim=-1)\n",
        "            output_cap = torch.argmax(output_cap, dim=-1)\n",
        "\n",
        "            output_punt_inicial = torch.softmax(output_punt_inicial, dim=-1)\n",
        "            output_punt_inicial = torch.argmax(output_punt_inicial, dim=-1)\n",
        "\n",
        "            output_punt_final = torch.softmax(output_punt_final, dim=-1)\n",
        "            output_punt_final = torch.argmax(output_punt_final, dim=-1)\n",
        "\n",
        "        return output_cap, output_punt_inicial, output_punt_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iPyTe1gYhO-v"
      },
      "outputs": [],
      "source": [
        "# Cargar el dataframe y crear dataloader\n",
        "df = pd.read_parquet(\"/content/drive/MyDrive/TP AA II/datos_modelo.parquet\")\n",
        "\n",
        "dataset = dataset_secuencias(df,30000)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "\n",
        "valid_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader  = DataLoader(valid_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\",device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg4HU6PfAEnh",
        "outputId": "3b938221-c01d-4418-957d-6d06321e5698"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b7vH74JvhO-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4675e01e-ed79-448f-f151-501fe9ef46d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Crear Modelo\n",
        "model = Uni_RNN(\n",
        "    hidden1=32,\n",
        "    hidden2=16\n",
        ")\n",
        "\n",
        "model=model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "ce  = nn.CrossEntropyLoss(ignore_index=-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zVLhLyByhO-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b3402a-a028-4983-a039-530751812b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Train Loss = 1.0933 | Val Loss = 0.8752\n",
            "Epoch 2/3 | Train Loss = 0.8019 | Val Loss = 0.8100\n",
            "Epoch 3/3 | Train Loss = 0.7388 | Val Loss = 0.7998\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "\n",
        "num_epoch=1 #poner en 3\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for seqs, caps, pini, pfin, lengths in train_loader:\n",
        "\n",
        "        seqs = seqs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        pini = pini.to(device)\n",
        "        pfin = pfin.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        out_cap, out_pini, out_pfin = model(seqs, lengths)\n",
        "\n",
        "        # Pérdida capitalización (multiclase)\n",
        "        loss_cap = ce(\n",
        "            out_cap.reshape(-1, 4),\n",
        "            caps.reshape(-1)\n",
        "        )\n",
        "\n",
        "        # Pérdida puntuación inicial\n",
        "        loss_pini  = ce(\n",
        "            out_pini.reshape(-1, 2),\n",
        "            pini.reshape(-1)\n",
        "        )\n",
        "\n",
        "        # Pérdida puntuación final (multiclase)\n",
        "        loss_pfin = ce(\n",
        "            out_pfin.reshape(-1, 4),\n",
        "            pfin.reshape(-1)\n",
        "        )\n",
        "\n",
        "\n",
        "        loss = loss_cap + loss_pini + loss_pfin\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()/len(train_loader)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for seqs, caps, pini, pfin, lengths in valid_loader:\n",
        "\n",
        "            seqs = seqs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            pini = pini.to(device)\n",
        "            pfin = pfin.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "\n",
        "            out_cap, out_pini, out_pfin = model(seqs, lengths)\n",
        "\n",
        "            # capitalización\n",
        "            loss_cap = ce(\n",
        "                out_cap.reshape(-1, 4),\n",
        "                caps.reshape(-1)\n",
        "            )\n",
        "\n",
        "            # Pérdida puntuación inicial\n",
        "            loss_pini  = ce(\n",
        "            out_pini.reshape(-1, 2),\n",
        "            pini.reshape(-1)\n",
        "            )\n",
        "\n",
        "            # puntuación final\n",
        "            loss_pfin = ce(\n",
        "                out_pfin.reshape(-1, 4),\n",
        "                pfin.reshape(-1)\n",
        "            )\n",
        "\n",
        "\n",
        "            loss = loss_cap + loss_pini + loss_pfin\n",
        "            val_loss += loss.item()/len(valid_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epoch} | Train Loss = {total_loss:.4f} | Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decodificador(data,lengths,model):\n",
        "  \"\"\"Con los tokens del input y el modelo podemos reconstruir el texto \"\"\"\n",
        "  from statistics import mode\n",
        "  pinicial, pfinal , cap= model(data,lengths,inferencia=True) #listas que me dicen que poner\n",
        "\n",
        "\n",
        "  if len(data.shape) == 1:\n",
        "    pinicial = pinicial.unsqueeze(0)\n",
        "    pfinal = pfinal.unsqueeze(0)\n",
        "    cap = cap.unsqueeze(0)\n",
        "\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "  tokens= [tokenizer.convert_ids_to_tokens(data[i][:lengths[i]]) for i in range(data.shape[0]) ]\n",
        "\n",
        "\n",
        "  for elemento in range(data.shape[0]): # por cada elemento en el batch\n",
        "\n",
        "    i=0\n",
        "\n",
        "    pinicial_elemento= pinicial[elemento][:lengths[i]]\n",
        "    pfinal_elemento = pfinal[elemento][:lengths[i]]\n",
        "    cap_elemento = cap[elemento][:lengths[i]]\n",
        "    token_elemento=tokens[elemento][:lengths[i]]\n",
        "\n",
        "    texto_final =\"\"\n",
        "    palabra=\"\"\n",
        "    palabras_tokenizadas=[]\n",
        "    captializacion_palabras=[] #lista de subcaptilizaciones\n",
        "    subcapitalizacion=[]\n",
        "    while i <len(token_elemento):\n",
        "\n",
        "      if pinicial_elemento[i]<=0.5:\n",
        "        apertura=\"\"\n",
        "      else:\n",
        "        apertura=\"¿\"\n",
        "\n",
        "      if pfinal_elemento[i]==0:\n",
        "        cierre=\"\"\n",
        "      elif pfinal_elemento[i]==1:\n",
        "        cierre=\"?\"\n",
        "      elif pfinal_elemento[i]==2:\n",
        "        cierre=\".\"\n",
        "      elif pfinal_elemento[i]==3:\n",
        "        cierre=\",\"\n",
        "\n",
        "      #Si tiene ## se lo borro\n",
        "      if token_elemento[i][:2]==\"##\":\n",
        "        palabra+=apertura+token_elemento[i][2:]+cierre\n",
        "        subcapitalizacion.append(cap_elemento[i])\n",
        "\n",
        "      else:\n",
        "        palabra+=apertura+token_elemento[i]+cierre\n",
        "        subcapitalizacion.append(cap_elemento[i])\n",
        "\n",
        "\n",
        "\n",
        "      if i != len(token_elemento)-1 and token_elemento[i+1][:2]!=\"##\": #si el proximo token no es decontinuacion termine la palabra\n",
        "        palabras_tokenizadas.append(palabra)\n",
        "        palabra=\"\"\n",
        "        captializacion_palabras.append(subcapitalizacion)\n",
        "        subcapitalizacion=[]\n",
        "\n",
        "      elif i == len(token_elemento)-1:\n",
        "        palabras_tokenizadas.append(palabra)\n",
        "        captializacion_palabras.append(subcapitalizacion)\n",
        "      i+=1\n",
        "\n",
        "\n",
        "    texto_final =\"\"\n",
        "    for i in range(len(palabras_tokenizadas)):\n",
        "\n",
        "      #procesamiento de captializaciones\n",
        "      subcapitlizacion= captializacion_palabras[i]\n",
        "      if mode(subcapitalizacion)==0:\n",
        "        palabra_procesada=palabras_tokenizadas[i]\n",
        "\n",
        "      elif mode(subcapitalizacion)==1:\n",
        "        palabra_procesada=palabras_tokenizadas[i][0].upper()+palabras_tokenizadas[i][1:]\n",
        "\n",
        "      elif mode(subcapitalizacion)==2:\n",
        "        palabra_procesada=\"\"\n",
        "        for j in range(len(palabras_tokenizadas[i])):\n",
        "\n",
        "          if j % 2 == 0:\n",
        "            palabra_procesada+=palabras_tokenizadas[i][j]\n",
        "\n",
        "          else:\n",
        "            palabra_procesada+=palabras_tokenizadas[i][j].upper()\n",
        "\n",
        "      else:\n",
        "        palabra_procesada=palabras_tokenizadas[i].upper()\n",
        "\n",
        "\n",
        "      if i == 0:\n",
        "        texto_final+=palabra_procesada\n",
        "      else:\n",
        "        texto_final+=\" \"\n",
        "        texto_final+=palabra_procesada\n",
        "\n",
        "    print(texto_final)"
      ],
      "metadata": {
        "id": "tWu1GvrxCXTa"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_visualizacion=pd.read_parquet(\"/content/drive/MyDrive/TP AA II/visualizacion.parquet\")\n",
        "dataset_visualizacion=dataset_secuencias(df)\n",
        "visualizacion_loader = DataLoader(dataset_visualizacion, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "B3ST8PseDKGh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seqs, _ , _, _, lengths in visualizacion_loader:\n",
        "\n",
        "        seqs = seqs.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        decodificador(seqs,lengths,model)\n",
        "        break\n"
      ],
      "metadata": {
        "id": "B7ZpHWw6Fuk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f954c29-2522-41bb-c50b-6dd4e01b53f1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿sus experimentos ¿sí ¿eso es lo que me asusta el día en que nos prometimos ¿me habló de sus experimentos lba a descubrir algo tan asombroso que le haría dudar de su propia cordura ¿tenía algo raro en la mirada un halo de misterio sus palabras me entusiasmaron ¿jam¿ás ¿he dudado de él ¿pero me preocupa ¿no puedo evitarlo y ahora esta carta no puedo seguir con esta incertidumbre ¿he de saberlo ¿vi¿ctor le has visto ¿sí hace unas tres semanas ¿le vi paseando ¿solo por el bosque a mí también me habló de su trabajo ¿le pregunté si podía visitar su laboratorio ¿me miró furioso y dijo que no dejaba entrar a nadie se comportó de una forma muy extraña ¿qué? podemos hacer ¿podría estar enfermo ¿no te preocupes lré a ver al ¿dr ¿wa¿ld¿man fue profesor de ¿hen¿ry en la facultad ¿tal vez él pueda aclararme algo vi¿ctor eres un encanto ¿sabe¿s que por ti llegar¿ía al fin del mundo ¿no me gustaría ¿te aprecio demasiado o¿jal¿á fuera así\n",
            "¿ten¿go miedo la he leído una y otra vez ¿pero no entiendo ¿ni una palabra es¿cu¿cha debes tener fe en mí ¿eli¿za¿beth ¿sp¿era \" ¿mi trabajo es lo más importante ¿incluso más que ú \" ¿por las noches el viento aúlla en las ontañas \" ¿aquí no hay ad¿ie \" ¿ningún entrometido des¿cubrirá mi ecreto \" ¿qué? querrá decir ¿qué? más dice\n",
            "¿HeR¿R ¿FrA¿NkEn¿sTeIn eS uN jOvEn tAn bRiLlAnTe cOmO iMpReViSiBlE ¿Me tIeNe pErPlEjO ¿EsTo¿y pReOcUpAdA pOr ¿HeN¿Ry pOr qUé hA dEjAdO lA uNiVeRsIdAd ¿Le iBa mUy bIeN y pArEcÍa cOnTeNtO cOn sU tRaBaJo\n",
            "¿sus estudios del galvanismo químico y la electrobiología estaban muy por encima de las teorías de nuestra facultad de hecho habían llegado a una fase muy avanzada ¿se estaban convirtiendo en un peligro\n",
            "¿ViVo eN uNa aTaLaYa aBaNdOnAdA cErCa dE lA vIlLa dE oLdStAdT \" ¿SóLo tEnGo a uN aYuDaNtE pArA mIs xPeRiMeNtOs \"\n",
            "¿HaS? tEnIdO nOtIcIaS dE ¿HeN¿Ry ¿Sí yA hAcÍa cUaTrO mEsEs ¿AcAbA dE lLeGaR vI¿CtOr tIeNeS qUe aYuDaRmE pOr sUpUeStO qUe sÍ\n",
            "¿buenas noches ¿vi¿ctor y gracias ¿buenas noches y no te preocupes ¿pro¿meti¿do des¿cu¿ida\n",
            "¿Vo¿y cOnTiGo ¿Ni hAbLaR ! ¿TeN¿Go qUe hAcErLo ¿DaMe uN mInUtO\n",
            "¿tiene el cuello roto ¿el cerebro ¿no sirve\n",
            "¿EsTá sAlIeNdO lA lUnA ¿No pOdEmOs pErDeR tIeMpO\n",
            "¿está descansando ¿esperando una nueva vida\n",
            "¿gracias ¿ca¿ball¿eros la clase ha terminado\n",
            "¿me Alegro Mucho De Verte ¿qué? Te Ocurre\n",
            "¿No vAmOs ¿No pUeDe hAcErTe nAdA\n",
            "¿vi¿ctor lo siento\n",
            "¿vi¿ctor ¿qué?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seqs[4].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR6uhYEJ8wty",
        "outputId": "89e43120-0416-4b2a-df8d-6e65b893c1cd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([239])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}