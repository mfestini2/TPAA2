{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9d45b9-7e02-4e29-860d-4653daf07a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099055e9-6304-402e-bd5a-e2a070d4d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Uni_RNN(nn.Module):\n",
    "    #hidden1: chatpgt 512-384\n",
    "    #hidden2: chatpgt 256-128\n",
    "    def __init__(self,embedding_model,hidden1,hidden2):\n",
    "        super().__init__()\n",
    "        self.embedding_dim=768\n",
    "        \n",
    "        bert= BertModel.from_pretrained(embedding_model)\n",
    "        self.embedding_vector = bert.embeddings.word_embeddings.weight\n",
    "        self.LSTM= nn.LSTM(self.embedding_dim,hidden1, batch_first=True)\n",
    "        self.LSTM2= nn.LSTM(hidden1,hidden2, batch_first=True)\n",
    "        self.fc_pinicial= nn.Linear(hidden2,1)\n",
    "        self.fc_cap = nn.Linear(hidden2, 1)         # sigmoid\n",
    "        self.fc_pfinal = nn.Linear(hidden2, 3) \n",
    "                                           \n",
    "    def forward(self,data,inferencia=False):\n",
    "        \n",
    "        data= self.embedding_vector[data].reshape(data.shape[1],self.embedding_dim)\n",
    "        data, _=self.LSTM(data)\n",
    "        data, _=self.LSTM2(data)\n",
    "        final= data\n",
    "\n",
    "        output_cap= self.fc_cap(final)\n",
    "        output_punt_inicial=self.fc_pinicial(final)\n",
    "        output_punt_final=self.fc_pfinal(final)\n",
    "\n",
    "        if inferencia:\n",
    "            output_punt_final=F.softmax(self.fc_pfinal(final))\n",
    "            output_punt_final=torch.argmax(punt_final_logits,dim=-1)\n",
    "            output_cap= torch.sigmoid(self.fc_cap(final))\n",
    "            output_punt_inicial= torch.sigmoid(self.fc_pinicial(final))\n",
    "            \n",
    "        return output_cap,output_punt_inicial,output_punt_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93145e-8263-487b-a8b4-52073db86f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class dataset_subtitulos(Dataset):\n",
    "    def __init__(self, path):\n",
    "        data = pd.read_parquet(\n",
    "            path,\n",
    "            columns=[\n",
    "                \"instancia_id\",\n",
    "                \"token_id\",\n",
    "                \"punt_inicial\",\n",
    "                \"puntos\",\n",
    "                \"comas\",\n",
    "                \"cierre_pregunta\",\n",
    "                \"capitalizacion\",\n",
    "            ],\n",
    "        )\n",
    "        self.instancias = [g for _, g in data.groupby(\"instancia_id\")]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instancias)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        df_instancia=self.instancias[idx]\n",
    "        features= torch.tensor(df_instancia[\"token_id\"].values)\n",
    "        target = torch.tensor(df_instancia[[\"punt_inicial\",\"puntos\",\"comas\",\"cierre_pregunta\",\"capitalizacion\"]].values)\n",
    "        return features, target\n",
    "\n",
    "sub_data = dataset_subtitulos(\"datos_modelo.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "#data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5015d-cbc9-433e-9f2e-d15f03259849",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5caca-69fb-40cd-ad17-f7174a24ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=int(len(sub_data)*0.8)\n",
    "valid_size=len(sub_data)-train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(sub_data,[train_size,valid_size])\n",
    "\n",
    "data_loader_train=DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "data_loader_valid=DataLoader(valid_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87dbb9-2277-49d9-a70e-1321e1031862",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in data_loader_train:\n",
    "    break\n",
    "model=Uni_RNN(\"bert-base-multilingual-cased\",512,256)\n",
    "bert= BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "embedding_vector = bert.embeddings.word_embeddings.weight\n",
    "print(data.shape)\n",
    "model.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc2414-6a7d-40ad-aac6-94db5747317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING LOOP\n",
    "data_loader_validad=[]\n",
    "model=Uni_RNN(\"bert-base-multilingual-cased\",512,256)\n",
    "bce=nn.BCEWithLogitsLoss()\n",
    "ce = nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "num_epoch=5\n",
    "train_losses =[]\n",
    "val_losses =[]\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    running_loss=0.0\n",
    "    for data, target in data_loader_train:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cap_target= target[:,3]\n",
    "        pinicial_target=target[:,0]\n",
    "        pfinal_target=target[:,1:3]\n",
    "        \n",
    "        cap_output ,pinicial_output, pfinal_output=model(data)\n",
    "        loss_cap= bce(cap_output,cap_target)\n",
    "        loss_pinicial=bce(pinicial_output,pinicial_target)\n",
    "        loss_pfinal=ce(pfinal_output,pfinal_target)\n",
    "        loss=loss_cap+loss_pinicial+loss_pfinal #esto es todo un tensor\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss=0.0\n",
    "    with torch.inference_mode():\n",
    "        for data, target in data_loader_valid:\n",
    "            cap_target= target[:,3]\n",
    "            pinicial_target=target[:,0]\n",
    "            pfinal_target=target[:,1:3]\n",
    "            \n",
    "            cap_output ,pinicial_output, pfinal_output=model(data)\n",
    "            \n",
    "            loss_cap= bce(cap_output,cap_target)\n",
    "            loss_pinicial=bce(pinicial_output,pinicial_target)\n",
    "            loss_pfinal=ce(pfinal_output,pfinal_target)\n",
    "            \n",
    "            loss=loss_cap+loss_pinicial+loss_pfinal #esto es todo un tensor\n",
    "            running_loss += loss.item()\n",
    "        val_loss=running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epoch} -Train loss: {train_loss} - Val loss {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615acc8-e863-49d9-86aa-04e1a0bc08db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
